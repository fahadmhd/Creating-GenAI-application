{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5686392e-a3a8-47eb-b0f4-6e8a7124f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09cc4bf-b910-44cd-9f4e-d35604f842dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fahad\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10da265c-ca74-4b60-aebd-86cb60c5c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the text document\n",
    "def load_text_file(file_path):  #encoding='utf-8'\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# 2. Prepare the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c07e965-8783-41a7-95cd-75926fcc419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(text, tokenizer, block_size=128):\n",
    "    \"\"\"\n",
    "    Prepare a tokenized dataset from the input text.\n",
    "    Splits the text into chunks of `block_size` and tokenizes each chunk.\n",
    "    \"\"\"\n",
    "    # Split the text into chunks of block_size\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "    input_ids = tokenized[\"input_ids\"].squeeze()\n",
    "\n",
    "    # Create chunks\n",
    "    chunked_input_ids = [\n",
    "        input_ids[i:i + block_size]\n",
    "        for i in range(0, len(input_ids), block_size)\n",
    "        if len(input_ids[i:i + block_size]) == block_size\n",
    "    ]\n",
    "\n",
    "    # Convert chunks to Dataset format\n",
    "    dataset = Dataset.from_dict({\"input_ids\": chunked_input_ids})\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e602edb6-90f0-4438-a158-90c51b55eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_gpt2(dataset, model, tokenizer, output_dir=\"./model\"):\n",
    "    \"\"\"\n",
    "    Fine-tune the GPT-2 model on a given dataset.\n",
    "    \"\"\"\n",
    "    # Check the dataset\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"The dataset is empty. Ensure the input text is correctly processed.\")\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"no\",\n",
    "        learning_rate=5e-5,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_first_step=True,\n",
    "        report_to=\"none\"  # Disable reporting to external services\n",
    "    )\n",
    "\n",
    "    # Define data collator for padding\n",
    "    def data_collator(features):\n",
    "        batch = tokenizer.pad(features, padding=True, return_tensors=\"pt\")\n",
    "        return {\"input_ids\": batch[\"input_ids\"], \"labels\": batch[\"input_ids\"]}\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine-tuned model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "# 3. Fine-tune GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a4d361b-76b3-4b96-a8dc-828ff8d6304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_qa(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Interact with the fine-tuned model by asking questions.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Start Asking Questions (type 'quit' to exit) ---\")\n",
    "    while True:\n",
    "        question = input(\"Q: \")\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        inputs = tokenizer.encode(question, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31dcb44-9a3f-4e12-aefd-9a987fc567b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is the the name of  the capital city of Pakistan?\n",
      "Answer: Islamabad (City of Islam) is the capital city of Pakistan.[9] It is the country's tenth-most populous city\n",
      "with a population of 1,108,872 people[5][10] and is federally administered by the Pakistani government\n",
      "as part of the Islamabad Capital Territory. Built as a planned city in the 1960s and established in 1967,\n",
      "it replaced Karachi as Pakistan's national capital.\n",
      "Question: Who made the master plan of Islamabad?\n",
      "Answer: The Greek architect Constantinos Apostolou Doxiadis developed Islamabad's master plan, in which he\n",
      "divided it into eight zones; the city comprises administrative, diplomatic enclave, residential areas,\n",
      "educational and industrial sectors, commercial areas, as well as rural and green areas administered\n",
      "by the Islamabad Metropolitan Corporation with support from the Capital Development Authority.\n",
      "Islamabad is known for its parks and forests, including the Margalla Hills National\n",
      "Park and the Shakarparian.[11] It is home to several landmarks, including the \n",
      "country's flagship Faisal Mosque, which is the world's fifth-largest mosque. Other prominent landmarks include the Pakistan\n",
      "Monument and Democracy Square.[12][13][14]\n",
      "Question: What is the costs of living in islamabad?\n",
      "Answer:Rated as Gamma + by the Globalization and World Cities Research Network,[15] Islamabad has one of the highest costs of living\n",
      "in Pakistan. The city's populace is dominated by both middle and upper-middle class citizens.[16][17] Islamabad is home to twenty\n",
      "universities, including Bahria University, Quaid-e-Azam University, PIEAS, COMSATS University, and NUST.[18] It is also rated as one\n",
      "of the safest cities in Pakistan and has an expansive RFID-enabled surveillance system with almost 2,000 active CCTV cameras.[19][20]\n",
      "Question: What is Toponymy of ilamabad?\n",
      "Anwer:The name Islamabad means City of Islam. It is derived from two words: Islam and abad. Islam refers to the religion of Islam,\n",
      "Pakistan's state religion, and -abad is a Persian suffix meaning cultivated place, indicating an inhabited place or city.[21] \n",
      "According to a history book by Muhammad Ismail Zabeeh, teacher and poet Qazi Abdur Rehman Amritsari proposed the name of the city.[22][23]\n",
      "Question: What is abbreviated of Islamabad?\n",
      "Occasionally in writing, Islamabad is colloquially abbreviated ISB. Such usage originated in SMS language,\n",
      "in part due to the IATA location identifier for the Islamabad International Airport.\n",
      "Question: what is History of islamabad?\n",
      "Answer:Early history\n",
      "Islamabad Capital Territory, located on the Pothohar Plateau of the northern Punjab region, is considered one of the earliest\n",
      "sites of human settlement in Asia.[24] Some of the earliest Stone Age artefacts in the world have been found on the plateau, dating \n",
      "from 100,000 to 500,000 years ago. Rudimentary stones recovered from the terraces of the Soan River testify to the endeavours of early man in the\n",
      "inter-glacial period.[25] Items of pottery and utensils dating back to prehistory have been found.[26]\n",
      "Question:Who is Abdul Ghafoor?\n",
      "Answer:Excavations by Dr. Abdul Ghafoor Lone reveal evidence of a prehistoric culture in the area. Relics and human skulls have been\n",
      "found dating back to 5000 BCE that indicate the region was home to Neolithic peoples who settled on the banks of the Soan[24] and who later developed small communities in the region around 3000 BCE.[25][27]\n",
      "Question: how is Indus Valley civilization flourished?\n",
      "Anwser:The Indus Valley civilization flourished in the region between the 23rd and 18th centuries BCE. Later the area was an early\n",
      "settlement of the Aryan community which migrated into the region from Central Asia.[24] Many great armies such as those of Zahiruddin Babur,\n",
      "Genghis Khan, Timur and Ahmad Shah Durrani crossed the region during their invasions of the Indian subcontinent.[24][28] In 2015â€“16,\n",
      "the Federal Department of Archaeology and Museums, with the financial support of National Fund for Cultural Heritage, carried out initial\n",
      "archaeological excavations in which unearthed the remains of a Buddhist stupa at Ban Faqiran, near the Shah Allah Ditta caves, which was \n",
      "dated to the 2nd to the 5th century CE.[29]\n",
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 7\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahad\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 08:35, Epoch 2.86/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>78.042400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the pretrained GPT-2 model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    \n",
    "    # Load your text file\n",
    "    text = load_text_file(\"koedia_custom_dataset.txt\")\n",
    "    print(text)\n",
    "    # Prepare dataset\n",
    "    dataset = prepare_dataset(text, tokenizer)\n",
    "\n",
    "    print(dataset)\n",
    "    # Fine-tune the model\n",
    "    fine_tune_gpt2(dataset, model, tokenizer)\n",
    "\n",
    "    # Use the fine-tuned model for interactive Q&A\n",
    "    interactive_qa(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07defe78-92ff-40c0-b787-a0972fda8a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717ca86-c11f-4ebf-92f0-5d01a058bc86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
